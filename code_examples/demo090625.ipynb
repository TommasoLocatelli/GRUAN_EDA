{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254cd048",
   "metadata": {},
   "source": [
    "# GRUANpy Proof of Concept\n",
    "\n",
    "GRUANpy is a python toolkit to facilitate the analysis of GRUAN data. It includes several functionalities and is easily extensible thanks to its structure based on the inclusion of different data models and helper classes that provide specialized methods for different types of purposes. The different functions can also be executed in succession in order to create real data pipelines that allow a large variety of outputs.\n",
    "\n",
    "Here is a list of the main features identified so far:\n",
    "- download, the ability to download data of interest\n",
    "- merge, the ability to merge data from different products\n",
    "- aggregation, the ability to aggregate different observations and lower the resolution of the data (respecting the GRUAN principles in uncertainty processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edef7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gruanpy import GRUANpy\n",
    "from datetime import datetime\n",
    "download_folder=r\"gdp_demo_090625\"\n",
    "gp = GRUANpy(download_folder=download_folder)\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d539f1",
   "metadata": {},
   "source": [
    "## DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab868b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 files in pub/data/gruan/processing/level2/RS92-GDP/version-002/LIN/2018\n",
      "LIN-RS-01_2_RS92-GDP_002_20180611T180000_1-002-001.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180103T000000_1-002-002.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180612T002400_1-000-002.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180122T120000_1-002-001.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180613T180000_1-002-002.nc\n",
      "--------------------------------------------------\n",
      "Downloading first 2 files for demo purpose\n",
      "Download completed.\n",
      "--------------------------------------------------\n",
      "     Attribute                                              Value\n",
      "0  Conventions                                             CF-1.4\n",
      "1        title                RS92 GRUAN Data Product (Version 2)\n",
      "2  institution  MOL - Lindenberg Meteorological Observatory; D...\n",
      "3       source                                           RS92-SGP\n",
      "4      history  2018-06-11 21:30:51.000Z RS92-GDP: RS92 GRUAN ...\n",
      "--------------------------------------------------\n",
      "     Attribute                                              Value\n",
      "0  Conventions                                             CF-1.4\n",
      "1        title                RS92 GRUAN Data Product (Version 2)\n",
      "2  institution  MOL - Lindenberg Meteorological Observatory; D...\n",
      "3       source                                           RS92-SGP\n",
      "4      history  2018-01-03 12:37:10.000Z RS92-GDP: RS92 GRUAN ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download Gruan Data Product (GDP) through NOAA FTP\n",
    "ftp_dir_path=r'pub/data/gruan/processing/level2/RS92-GDP/version-002/LIN/2018'\n",
    "files=gp.search(ftp_dir_path=ftp_dir_path)\n",
    "print(f\"Found {len(files)} files in {ftp_dir_path}\")\n",
    "for file in files[:5]:\n",
    "    print(file)\n",
    "print(\"-----\"*10)\n",
    "print(\"Downloading first 2 files for demo purpose\")\n",
    "files=files[:2]\n",
    "for file in files:\n",
    "    gp.download(ftp_dir_path=ftp_dir_path, file_name=file)\n",
    "print(\"Download completed.\")\n",
    "print(\"-----\"*10)\n",
    "for file in files:\n",
    "    gdp=gp.read(download_folder+r'/ '[:-1]+file)\n",
    "    print(gdp.global_attrs.head())\n",
    "    print(\"-----\"*10)\n",
    "\n",
    "# Iterative script at code_examples\\download_gdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2fce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing API request ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 09:40:04,409 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-09 09:40:04,826 INFO Request ID is c8fa3021-8d00-4a0c-96ea-f3bb487f7ab7\n",
      "2025-06-09 09:40:04,933 INFO status has been updated to accepted\n",
      "2025-06-09 09:40:19,556 INFO status has been updated to running\n",
      "2025-06-09 09:41:21,705 INFO status has been updated to successful\n",
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Download GRUAN DATA through CDS API in csv format\n",
    "\n",
    "api_response_file = \"api_response.csv\"\n",
    "api_request = \"\"\"\n",
    "import cdsapi\n",
    "\n",
    "dataset = \"insitu-observations-gruan-reference-network\"\n",
    "request = {\n",
    "    \"variable\": [\n",
    "        \"air_temperature\",\n",
    "        \"relative_humidity\",\n",
    "        \"relative_humidity_effective_vertical_resolution\",\n",
    "        \"wind_speed\",\n",
    "        \"wind_from_direction\",\n",
    "        \"eastward_wind_speed\",\n",
    "        \"northward_wind_speed\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"air_pressure\",\n",
    "        \"altitude\",\n",
    "        \"geopotential_height\",\n",
    "        \"frost_point_temperature\",\n",
    "        \"water_vapour_volume_mixing_ratio\",\n",
    "        \"vertical_speed_of_radiosonde\",\n",
    "        \"time_since_launch\"\n",
    "    ],\n",
    "    \"year\": \"2014\",\n",
    "    \"month\": \"10\",\n",
    "    \"day\": [\"14\"],\n",
    "    \"data_format\": \"csv\",\n",
    "    \"area\": [90, 0, 0, 90]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\"\"\"+f\"\"\"\n",
    "target= r\"{download_folder}\\\\{api_response_file}\" # Change this to your desired output path\n",
    "client.retrieve(dataset, request, target)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Executing API request ...\")\n",
    "gp.exec_request(api_request)\n",
    "print(\"-----\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3897e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 09:42:19,946 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing API request ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 09:42:20,308 INFO Request ID is d183ea59-8790-4387-a504-72fdcf6f9632\n",
      "2025-06-09 09:42:20,513 INFO status has been updated to accepted\n",
      "2025-06-09 09:42:42,525 INFO status has been updated to running\n",
      "2025-06-09 09:43:11,198 INFO status has been updated to successful\n",
      "                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Download GRUAN DATA through CDS API in netCDF format\n",
    "\n",
    "api_response_file = \"api_response.nc\"\n",
    "api_request = \"\"\"\n",
    "import cdsapi\n",
    "\n",
    "dataset = \"insitu-observations-gruan-reference-network\"\n",
    "request = {\n",
    "    \"variable\": [\n",
    "        \"air_temperature\",\n",
    "        \"relative_humidity\",\n",
    "        \"relative_humidity_effective_vertical_resolution\",\n",
    "        \"wind_speed\",\n",
    "        \"wind_from_direction\",\n",
    "        \"eastward_wind_speed\",\n",
    "        \"northward_wind_speed\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"air_pressure\",\n",
    "        \"altitude\",\n",
    "        \"geopotential_height\",\n",
    "        \"frost_point_temperature\",\n",
    "        \"water_vapour_volume_mixing_ratio\",\n",
    "        \"vertical_speed_of_radiosonde\",\n",
    "        \"time_since_launch\"\n",
    "    ],\n",
    "    \"year\": \"2014\",\n",
    "    \"month\": \"10\",\n",
    "    \"day\": [\"14\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"area\": [90, 0, 0, 90]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\"\"\"+f\"\"\"\n",
    "target= r\"{download_folder}\\\\{api_response_file}\" # Change this to your desired output path\n",
    "client.retrieve(dataset, request, target)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Executing API request ...\")\n",
    "gp.exec_request(api_request)\n",
    "print(\"-----\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff32d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:40:19,478 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-09 14:40:19,793 INFO Request ID is 2e65ec27-4bc5-41f6-8038-d09b6dc74491\n",
      "2025-06-09 14:40:19,885 INFO status has been updated to accepted\n",
      "2025-06-09 14:40:30,056 INFO status has been updated to running\n",
      "2025-06-09 14:40:42,842 INFO status has been updated to successful\n",
      "                                                                                         \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eb554bddba46166f9583f056b31a794b.nc'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cdsapi\n",
    "\n",
    "dataset = \"insitu-observations-gruan-reference-network\"\n",
    "request = {\n",
    "    \"variable\": [\n",
    "        \"air_temperature\",\n",
    "        \"relative_humidity\",\n",
    "        \"relative_humidity_effective_vertical_resolution\",\n",
    "        \"wind_speed\",\n",
    "        \"wind_from_direction\",\n",
    "        \"eastward_wind_speed\",\n",
    "        \"northward_wind_speed\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"air_pressure\",\n",
    "        \"altitude\",\n",
    "        \"geopotential_height\",\n",
    "        \"frost_point_temperature\",\n",
    "        \"time_since_launch\",\n",
    "        \"vertical_speed_of_radiosonde\",\n",
    "        \"water_vapour_volume_mixing_ratio\"\n",
    "    ],\n",
    "    \"year\": \"2020\",\n",
    "    \"month\": \"01\",\n",
    "    \"day\": [\n",
    "        \"03\", \"10\", \"15\",\n",
    "        \"17\", \"22\", \"23\",\n",
    "        \"24\", \"28\", \"29\",\n",
    "        \"31\"\n",
    "    ],\n",
    "    \"data_format\": \"netcdf\"\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request).download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5ed3b",
   "metadata": {},
   "source": [
    "# MERGE and AGGREGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f77b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDP RS41 spatial gridding\n",
    "file_path = r'C:\\Users\\tomma\\Documents\\SDC\\Repos\\GRUAN_EDA\\gdp\\data_examples\\LIN-RS-01_2_RS41-GDP_001_20141209T120000_1-009-002.nc'\n",
    "gdp=gp.read(file_path)\n",
    "bin_column = 'press'\n",
    "target_columns = ['temp', 'rh']\n",
    "ggd = gp.spatial_gridding(gdp, bin_column, target_columns)\n",
    "\n",
    "# Plot original and gridded data\n",
    "for column in target_columns:\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 6))\n",
    "    if bin_column == 'press':\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.invert_yaxis()\n",
    "    ax1.scatter(gdp.data[column], gdp.data[bin_column], label='Original Data', alpha=0.5)\n",
    "    ax1.scatter(ggd.data[column], ggd.data[bin_column], label='Gridded Data', color='red', alpha=0.5)\n",
    "    ax1.fill_betweenx(gdp.data[bin_column], gdp.data[column] - gdp.data[column+'_uc'], gdp.data[column] + gdp.data[column+'_uc'], color='blue', alpha=0.2, label='Original Uncertainty')\n",
    "    ax1.fill_betweenx(ggd.data[bin_column], ggd.data[column] - ggd.data[column+'_uc'], ggd.data[column] + ggd.data[column+'_uc'], color='red', alpha=0.2, label='Gridded Uncertainty')\n",
    "    ax1.set_xlabel(f'{column.capitalize()} {gdp.variables_attrs[gdp.variables_attrs['variable'] == column]['units'].values[0]}')\n",
    "    ax1.set_ylabel(f'{bin_column.capitalize()} {gdp.variables_attrs[gdp.variables_attrs['variable'] == bin_column]['units'].values[0]}')\n",
    "    ax1.legend()\n",
    "    long_name= gdp.variables_attrs[gdp.variables_attrs['variable'] == column]['long_name'].values[0]\n",
    "    ax1.set_title(f'{long_name} Spatial Gridding at Mandatory Levels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1559fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GDPs:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GDPs: 100%|██████████| 500/500 [25:02<00:00,  3.01s/it]\n",
      "Spatial Gridding:  10%|▉         | 49/500 [00:01<00:16, 27.27it/s]c:\\Users\\tomma\\Documents\\SDC\\Repos\\GRUAN_EDA\\gruanpy\\helpers\\grid\\gridding_manager.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambda x: (((x-x.mean())**2).sum()/(len(x)*(len(x)-1)))**0.5\n",
      "c:\\Users\\tomma\\Documents\\SDC\\Repos\\GRUAN_EDA\\gruanpy\\helpers\\grid\\gridding_manager.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambda x: (((x-x.mean())**2).sum()/(len(x)*(len(x)-1)))**0.5\n",
      "Spatial Gridding: 100%|██████████| 500/500 [00:15<00:00, 32.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# GDP RS41 temporal gridding\n",
    "\n",
    "# Read multiple GDP files\n",
    "gdp_folder=r'C:\\Users\\tomma\\Documents\\SDC\\Repos\\GRUAN_EDA\\gdp\\products_RS41-GDP-1_LIN_2017'\n",
    "gdp_files = [os.path.join(gdp_folder, f) for f in os.listdir(gdp_folder) if f.endswith('.nc')]\n",
    "gdps=[]\n",
    "for file in tqdm(gdp_files[:500] , desc=\"Reading GDPs\"):\n",
    "    gdps.append(gp.read(file))\n",
    "\n",
    "# Uniform Spatial gridding accross multiple GDPs\n",
    "ggds=[]\n",
    "target_columns = ['temp', 'rh']\n",
    "for gdp in tqdm(gdps, desc=\"Spatial Gridding\"):\n",
    "    ggd = gp.spatial_gridding(gdp, 'press', target_columns, mandatory_levels_flag=True)\n",
    "    ggds.append(ggd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b82d5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Gridded Data: 100%|██████████| 500/500 [00:01<00:00, 468.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Merge all gridded data\n",
    "mggd=pd.DataFrame()\n",
    "for ggd in tqdm(ggds, desc=\"Merging Gridded Data\"):\n",
    "    start_time_str = ggd.metadata[ggd.metadata['Attribute'] == 'g.Measurement.StartTime']['Value'].values[0]\n",
    "    start_time = datetime.strptime(start_time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    ggd_data = ggd.data.copy()\n",
    "    ggd_data['time'] = start_time\n",
    "    mggd = pd.concat([mggd, ggd_data], ignore_index=True)\n",
    "\n",
    "# Plotting the merged gridded data for one mandatory level\n",
    "for column in target_columns:\n",
    "    mand_lvl_val = 1000\n",
    "    mggd_at_lvl = mggd[mggd['mand_lvl'] == mand_lvl_val]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(mggd_at_lvl['time'], mggd_at_lvl[column], marker='o', linestyle='-', label=f'{column}')\n",
    "    ax.fill_between(mggd_at_lvl['time'], mggd_at_lvl[column] - mggd_at_lvl[column + '_uc'], mggd_at_lvl[column] + mggd_at_lvl[column + '_uc'], alpha=0.2, label='Uncertainty')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(f'{column.capitalize()} {gdp.variables_attrs[gdp.variables_attrs['variable'] == column]['units'].values[0]}')\n",
    "    ax.set_title(f'{column.capitalize()} at Mandatory Level {mand_lvl_val} Over Time')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920d7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal gridding from scratch DAY\n",
    "\n",
    "bin_size = 21 # Size of the time bin in days\n",
    "first_date = mggd['time'].min()\n",
    "mggd['day_diff'] = mggd['time'].apply(lambda x: (x-first_date).days) # distance in days\n",
    "mggd['time_bin'] = (mggd['day_diff'] // bin_size) * bin_size + bin_size / 2\n",
    "mggd['hour'] = mggd['time'].dt.hour\n",
    "dmggd = mggd[(mggd['hour'] >= 6) & (mggd['hour'] < 18)] #day merged gridded data\n",
    "\n",
    "tggd = dmggd.groupby(['mand_lvl', 'time_bin'])[target_columns].mean().reset_index() # 3.12\n",
    "tggd['time'] = pd.to_datetime(tggd['time_bin'], unit='D', origin=first_date)\n",
    "for col in target_columns:\n",
    "    tggd[col + '_uc_ucor_avg'] = dmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_ucor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.13\n",
    "    tggd[col + '_var'] = dmggd.groupby(['time_bin','mand_lvl'])[col].apply(\n",
    "                lambda x: ((((x-x.mean())**2).sum())/(len(x)*max((len(x)-1),1)))**0.5\n",
    "                ).reset_index(drop=True) #3.14\n",
    "    tggd[col + '_uc_sc']=dmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_scor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.15\n",
    "    tggd[col + '_uc_ucor']=(\n",
    "        tggd[col+'_uc_ucor_avg']**2 + tggd[col + '_var']**2 + tggd[col + '_uc_sc']**2)**0.5 #3.16\n",
    "    tggd[col + '_cor']=dmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_tcor'].mean().reset_index(drop=True) #3.17\n",
    "    tggd[col+'_uc']=(\n",
    "        tggd[col+'_uc_ucor']**2 + tggd[col+'_cor']**2)**0.5 #3.18\n",
    "    \n",
    "# Plotting the temporal gridded data\n",
    "for column in target_columns:\n",
    "    mand_lvl_val = 1000\n",
    "    tggd_at_lvl = tggd[tggd['mand_lvl'] == mand_lvl_val]\n",
    "    dmggd_at_lvl = dmggd[dmggd['mand_lvl'] == mand_lvl_val]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(dmggd_at_lvl['time'], dmggd_at_lvl[column], marker='x', linestyle='--', label=f'{column} (Original Day)')\n",
    "    ax.fill_between(dmggd_at_lvl['time'], dmggd_at_lvl[column] - dmggd_at_lvl[column + '_uc'], dmggd_at_lvl[column] + dmggd_at_lvl[column + '_uc'], alpha=0.1, label='Original Day Uncertainty')\n",
    "    ax.plot(tggd_at_lvl['time'], tggd_at_lvl[column], marker='o', linestyle='-', label=f'{column} (Temporal Gridded)')\n",
    "    ax.fill_between(tggd_at_lvl['time'], tggd_at_lvl[column] - tggd_at_lvl[column + '_uc'], tggd_at_lvl[column] + tggd_at_lvl[column + '_uc'], alpha=0.2, label='Uncertainty')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(f'{column.capitalize()} {gdp.variables_attrs[gdp.variables_attrs['variable'] == column]['units'].values[0]}')\n",
    "    ax.set_title(f'{column.capitalize()} at Mandatory Level {mand_lvl_val} Over Time (Temporal Gridded) at Day')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7319ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal gridding from scratch NIGHT\n",
    "\n",
    "bin_size = 21 # Size of the time bin in days\n",
    "first_date = mggd['time'].min()\n",
    "mggd['day_diff'] = mggd['time'].apply(lambda x: (x-first_date).days) # distance in days\n",
    "mggd['time_bin'] = (mggd['day_diff'] // bin_size) * bin_size + bin_size / 2\n",
    "mggd['hour'] = mggd['time'].dt.hour\n",
    "dmggd = mggd[(mggd['hour'] < 6) | (mggd['hour'] >= 18)] #day merged gridded data\n",
    "\n",
    "tggd = dmggd.groupby(['mand_lvl', 'time_bin'])[target_columns].mean().reset_index() # 3.12\n",
    "tggd['time'] = pd.to_datetime(tggd['time_bin'], unit='D', origin=first_date)\n",
    "for col in target_columns:\n",
    "    tggd[col + '_uc_ucor_avg'] = dmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_ucor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.13\n",
    "    tggd[col + '_var'] = dmggd.groupby(['time_bin','mand_lvl'])[col].apply(\n",
    "                lambda x: ((((x-x.mean())**2).sum())/(len(x)*max((len(x)-1),1)))**0.5\n",
    "                ).reset_index(drop=True) #3.14\n",
    "    tggd[col + '_uc_sc']=dmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_scor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.15\n",
    "    tggd[col + '_uc_ucor']=(\n",
    "        tggd[col+'_uc_ucor_avg']**2 + tggd[col + '_var']**2 + tggd[col + '_uc_sc']**2)**0.5 #3.16\n",
    "    tggd[col + '_cor']=dmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_tcor'].mean().reset_index(drop=True) #3.17\n",
    "    tggd[col+'_uc']=(\n",
    "        tggd[col+'_uc_ucor']**2 + tggd[col+'_cor']**2)**0.5 #3.18\n",
    "    \n",
    "# Plotting the temporal gridded data\n",
    "for column in target_columns:\n",
    "    mand_lvl_val = 1000\n",
    "    tggd_at_lvl = tggd[tggd['mand_lvl'] == mand_lvl_val]\n",
    "    dmggd_at_lvl = dmggd[dmggd['mand_lvl'] == mand_lvl_val]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(dmggd_at_lvl['time'], dmggd_at_lvl[column], marker='x', linestyle='--', label=f'{column} (Original Day)')\n",
    "    ax.fill_between(dmggd_at_lvl['time'], dmggd_at_lvl[column] - dmggd_at_lvl[column + '_uc'], dmggd_at_lvl[column] + dmggd_at_lvl[column + '_uc'], alpha=0.1, label='Original Day Uncertainty')\n",
    "    ax.plot(tggd_at_lvl['time'], tggd_at_lvl[column], marker='o', linestyle='-', label=f'{column} (Temporal Gridded)')\n",
    "    ax.fill_between(tggd_at_lvl['time'], tggd_at_lvl[column] - tggd_at_lvl[column + '_uc'], tggd_at_lvl[column] + tggd_at_lvl[column + '_uc'], alpha=0.2, label='Uncertainty')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(column.capitalize())\n",
    "    ax.set_title(f'{column.capitalize()} at Mandatory Level {mand_lvl_val} Over Time (Temporal Gridded) at Night')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f902d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
