{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254cd048",
   "metadata": {},
   "source": [
    "# GRUANpy Proof of Concept\n",
    "\n",
    "GRUANpy is a python toolkit to facilitate the analysis of GRUAN data. It includes several functionalities and is easily extensible thanks to its structure based on the inclusion of different data models and helper classes that provide specialized methods for different types of purposes. The different functions can also be executed in succession in order to create real data pipelines that allow a large variety of outputs.\n",
    "\n",
    "Here is a list of the main features identified so far:\n",
    "- download, the ability to download data of interest\n",
    "- merge, the ability to merge data from different products\n",
    "- aggregation, the ability to aggregate different observations and lower the resolution of the data (respecting the GRUAN principles in uncertainty processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edef7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gruanpy import GRUANpy\n",
    "from datetime import datetime\n",
    "download_folder=r\"gdp_demo_090625\"\n",
    "gp = GRUANpy(download_folder=download_folder)\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d539f1",
   "metadata": {},
   "source": [
    "## DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab868b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 files in pub/data/gruan/processing/level2/RS92-GDP/version-002/LIN/2018\n",
      "LIN-RS-01_2_RS92-GDP_002_20180611T180000_1-002-001.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180103T000000_1-002-002.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180612T002400_1-000-002.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180122T120000_1-002-001.nc\n",
      "LIN-RS-01_2_RS92-GDP_002_20180613T180000_1-002-002.nc\n",
      "--------------------------------------------------\n",
      "Downloading first 2 files for demo purpose\n",
      "Download completed.\n",
      "--------------------------------------------------\n",
      "     Attribute                                              Value\n",
      "0  Conventions                                             CF-1.4\n",
      "1        title                RS92 GRUAN Data Product (Version 2)\n",
      "2  institution  MOL - Lindenberg Meteorological Observatory; D...\n",
      "3       source                                           RS92-SGP\n",
      "4      history  2018-06-11 21:30:51.000Z RS92-GDP: RS92 GRUAN ...\n",
      "--------------------------------------------------\n",
      "     Attribute                                              Value\n",
      "0  Conventions                                             CF-1.4\n",
      "1        title                RS92 GRUAN Data Product (Version 2)\n",
      "2  institution  MOL - Lindenberg Meteorological Observatory; D...\n",
      "3       source                                           RS92-SGP\n",
      "4      history  2018-01-03 12:37:10.000Z RS92-GDP: RS92 GRUAN ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download Gruan Data Product (GDP) through NOAA FTP\n",
    "ftp_dir_path=r'pub/data/gruan/processing/level2/RS92-GDP/version-002/LIN/2018'\n",
    "files=gp.search(ftp_dir_path=ftp_dir_path)\n",
    "print(f\"Found {len(files)} files in {ftp_dir_path}\")\n",
    "for file in files[:5]:\n",
    "    print(file)\n",
    "print(\"-----\"*10)\n",
    "print(\"Downloading first 2 files for demo purpose\")\n",
    "files=files[:2]\n",
    "for file in files:\n",
    "    gp.download(ftp_dir_path=ftp_dir_path, file_name=file)\n",
    "print(\"Download completed.\")\n",
    "print(\"-----\"*10)\n",
    "for file in files:\n",
    "    gdp=gp.read(download_folder+r'/ '[:-1]+file)\n",
    "    print(gdp.global_attrs.head())\n",
    "    print(\"-----\"*10)\n",
    "\n",
    "# Iterative script at code_examples\\download_gdp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing API request ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 10:53:45,454 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-28 10:53:45,455 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-28 10:53:46,171 INFO Request ID is 1cee6683-13e9-4835-83c8-6c0deddbaf89\n",
      "2025-05-28 10:53:46,275 INFO status has been updated to accepted\n",
      "2025-05-28 10:55:02,255 INFO status has been updated to successful\n",
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Download GRUAN DATA through CDS API in csv format\n",
    "\n",
    "api_response_file = \"api_response.csv\"\n",
    "api_request = \"\"\"\n",
    "import cdsapi\n",
    "\n",
    "dataset = \"insitu-observations-gruan-reference-network\"\n",
    "request = {\n",
    "    \"variable\": [\n",
    "        \"air_temperature\",\n",
    "        \"relative_humidity\",\n",
    "        \"relative_humidity_effective_vertical_resolution\",\n",
    "        \"wind_speed\",\n",
    "        \"wind_from_direction\",\n",
    "        \"eastward_wind_speed\",\n",
    "        \"northward_wind_speed\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"air_pressure\",\n",
    "        \"altitude\",\n",
    "        \"geopotential_height\",\n",
    "        \"frost_point_temperature\",\n",
    "        \"water_vapour_volume_mixing_ratio\",\n",
    "        \"vertical_speed_of_radiosonde\",\n",
    "        \"time_since_launch\"\n",
    "    ],\n",
    "    \"year\": \"2014\",\n",
    "    \"month\": \"10\",\n",
    "    \"day\": [\"14\"],\n",
    "    \"data_format\": \"csv\",\n",
    "    \"area\": [90, 0, 0, 90]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\"\"\"+f\"\"\"\n",
    "target= r\"{download_folder}\\\\{api_response_file}\" # Change this to your desired output path\n",
    "client.retrieve(dataset, request, target)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Executing API request ...\")\n",
    "gp.exec_request(api_request)\n",
    "print(\"-----\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3897e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing API request ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:16:44,989 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-05-28 11:16:44,990 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2025-05-28 11:16:45,706 INFO Request ID is f5bfd683-46f5-40fb-bca0-50816aa87b21\n",
      "2025-05-28 11:16:45,809 INFO status has been updated to accepted\n",
      "2025-05-28 11:17:35,883 INFO status has been updated to successful\n",
      "                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Download GRUAN DATA through CDS API in netCDF format\n",
    "\n",
    "api_response_file = \"api_response.nc\"\n",
    "api_request = \"\"\"\n",
    "import cdsapi\n",
    "\n",
    "dataset = \"insitu-observations-gruan-reference-network\"\n",
    "request = {\n",
    "    \"variable\": [\n",
    "        \"air_temperature\",\n",
    "        \"relative_humidity\",\n",
    "        \"relative_humidity_effective_vertical_resolution\",\n",
    "        \"wind_speed\",\n",
    "        \"wind_from_direction\",\n",
    "        \"eastward_wind_speed\",\n",
    "        \"northward_wind_speed\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"air_pressure\",\n",
    "        \"altitude\",\n",
    "        \"geopotential_height\",\n",
    "        \"frost_point_temperature\",\n",
    "        \"water_vapour_volume_mixing_ratio\",\n",
    "        \"vertical_speed_of_radiosonde\",\n",
    "        \"time_since_launch\"\n",
    "    ],\n",
    "    \"year\": \"2014\",\n",
    "    \"month\": \"10\",\n",
    "    \"day\": [\"14\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"area\": [90, 0, 0, 90]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\"\"\"+f\"\"\"\n",
    "target= r\"{download_folder}\\\\{api_response_file}\" # Change this to your desired output path\n",
    "client.retrieve(dataset, request, target)\n",
    "\n",
    "\"\"\"\n",
    "print(\"Executing API request ...\")\n",
    "gp.exec_request(api_request)\n",
    "print(\"-----\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1d15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ce5ed3b",
   "metadata": {},
   "source": [
    "# MERGE and AGGREGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f77b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDP RS41 spatial gridding\n",
    "file_path = r'gdp_demo_090625\\LIN-RS-01_2_RS41-GDP_001_20141209T120000_1-009-002.nc'\n",
    "gdp=gp.read(file_path)\n",
    "\n",
    "bin_column = 'press' # Choose the binning column (alt or press)\n",
    "target_columns = ['temp', 'rh']\n",
    "\n",
    "ggd = gp.spatial_gridding(gdp, bin_column, target_columns)\n",
    "\n",
    "# Plot original and gridded data\n",
    "for column in target_columns:\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 6))\n",
    "    if bin_column == 'press':\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.invert_yaxis()\n",
    "    ax1.scatter(gdp.data[column], gdp.data[bin_column], label='Original Data', alpha=0.5)\n",
    "    ax1.scatter(ggd.data[column], ggd.data[bin_column], label='Gridded Data', color='red', alpha=0.5)\n",
    "    ax1.fill_betweenx(gdp.data[bin_column], gdp.data[column] - gdp.data[column+'_uc'], gdp.data[column] + gdp.data[column+'_uc'], color='blue', alpha=0.2, label='Original Uncertainty')\n",
    "    ax1.fill_betweenx(ggd.data[bin_column], ggd.data[column] - ggd.data[column+'_uc'], ggd.data[column] + ggd.data[column+'_uc'], color='red', alpha=0.2, label='Gridded Uncertainty')\n",
    "    ax1.set_xlabel(f'{column.capitalize()} {gdp.variables_attrs[gdp.variables_attrs['variable'] == column]['units'].values[0]}')\n",
    "    ax1.set_ylabel(f'{bin_column.capitalize()} {gdp.variables_attrs[gdp.variables_attrs['variable'] == bin_column]['units'].values[0]}')\n",
    "    ax1.legend()\n",
    "    long_name= gdp.variables_attrs[gdp.variables_attrs['variable'] == column]['long_name'].values[0]\n",
    "    ax1.set_title(f'{long_name} Mandatory Levels Spatial Gridding')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1559fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GDPs:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GDPs: 100%|██████████| 250/250 [09:02<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# GDP RS41 temporal gridding\n",
    "\n",
    "# Read multiple GDP files\n",
    "gdp_folder=r'C:\\Users\\tomma\\Documents\\SDC\\Repos\\GRUAN_EDA\\gdp\\products_RS41-GDP-1_LIN_2017'#gdp_demo_090625\\products_RS41-GDP-1_LIN_2017' # Path to the folder\n",
    "gdp_files = [os.path.join(gdp_folder, f) for f in os.listdir(gdp_folder) if f.endswith('.nc')]\n",
    "gdps=[]\n",
    "for file in tqdm(gdp_files[:250] , desc=\"Reading GDPs\"):\n",
    "    gdps.append(gp.read(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "988e5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spatial Gridding:  20%|██        | 50/250 [00:01<00:05, 37.13it/s]c:\\Users\\tomma\\Documents\\SDC\\Repos\\GRUAN_EDA\\gruanpy\\helpers\\grid\\gridding_manager.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambda x: (((x-x.mean())**2).sum()/(len(x)*(len(x)-1)))**0.5\n",
      "c:\\Users\\tomma\\Documents\\SDC\\Repos\\GRUAN_EDA\\gruanpy\\helpers\\grid\\gridding_manager.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  lambda x: (((x-x.mean())**2).sum()/(len(x)*(len(x)-1)))**0.5\n",
      "Spatial Gridding: 100%|██████████| 250/250 [00:06<00:00, 37.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Uniform Spatial gridding accross multiple GDPs\n",
    "ggds=[]\n",
    "target_columns = ['temp', 'rh']\n",
    "for gdp in tqdm(gdps, desc=\"Spatial Gridding\"):\n",
    "    ggd = gp.spatial_gridding(gdp, 'press', target_columns, mandatory_levels_flag=True)\n",
    "    ggds.append(ggd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b82d5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Gridded Data: 100%|██████████| 250/250 [00:00<00:00, 1042.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Merge all gridded data\n",
    "mggd=pd.DataFrame()\n",
    "for ggd in tqdm(ggds, desc=\"Merging Gridded Data\"):\n",
    "    start_time_str = ggd.metadata[ggd.metadata['Attribute'] == 'g.Measurement.StartTime']['Value'].values[0]\n",
    "    start_time = datetime.strptime(start_time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    ggd_data = ggd.data.copy()\n",
    "    ggd_data['time'] = start_time\n",
    "    mggd = pd.concat([mggd, ggd_data], ignore_index=True)\n",
    "\n",
    "# Plotting the merged gridded data\n",
    "for column in target_columns:\n",
    "    # Separate data into day and night based on time (e.g., 6:00-18:00 as day, else night)\n",
    "    mand_lvl_val = 1000\n",
    "    subset = mggd[mggd['mand_lvl'] == mand_lvl_val].copy()\n",
    "    subset['hour'] = subset['time'].dt.hour\n",
    "    day_data = subset[(subset['hour'] >= 6) & (subset['hour'] < 18)]\n",
    "    night_data = subset[(subset['hour'] < 6) | (subset['hour'] >= 18)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(day_data['time'], day_data[column], marker='o', linestyle='-', label=f'{column} (Day)')\n",
    "    ax.plot(night_data['time'], night_data[column], marker='x', linestyle='--', label=f'{column} (Night)')\n",
    "    ax.fill_between(day_data['time'], day_data[column] - day_data[column + '_uc'], day_data[column] + day_data[column + '_uc'], alpha=0.2, label='Day Uncertainty')\n",
    "    ax.fill_between(night_data['time'], night_data[column] - night_data[column + '_uc'], night_data[column] + night_data[column + '_uc'], alpha=0.2, label='Night Uncertainty')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(column.capitalize())\n",
    "    ax.set_title(f'{column.capitalize()} at Mandatory Level {mand_lvl_val} Over Time (Day vs Night)')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bbafb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbin_size = 7 #\\nfirst_date = mggd['time'].min()\\nmggd['day_diff'] = mggd['time'].apply(lambda x: (x-first_date).days) # distance in days\\nmggd['day_light'] = mggd['time'].dt.hour>=6 & mggd['time'].dt.hour<18\\nmggd['time_bin'] = (mggd['day_diff'] // bin_size) * bin_size + bin_size / 2\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PSEUDO CODE\n",
    "\"\"\"\n",
    "bin_size = 7 #\n",
    "first_date = mggd['time'].min()\n",
    "mggd['day_diff'] = mggd['time'].apply(lambda x: (x-first_date).days) # distance in days\n",
    "mggd['day_light'] = mggd['time'].dt.hour>=6 & mggd['time'].dt.hour<18\n",
    "mggd['time_bin'] = (mggd['day_diff'] // bin_size) * bin_size + bin_size / 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "920d7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal gridding from scratch\n",
    "\n",
    "bin_size = 21 # Size of the time bin in days\n",
    "first_date = mggd['time'].min()\n",
    "mggd['day_diff'] = mggd['time'].apply(lambda x: (x-first_date).days) # distance in days\n",
    "mggd['time_bin'] = (mggd['day_diff'] // bin_size) * bin_size + bin_size / 2\n",
    "\n",
    "# Filter mggd to only include rows where the hour is 12 (i.e., time is in the day)\n",
    "mggd['hour'] = mggd['time'].dt.hour\n",
    "dmggd = mggd[(mggd['hour'] >= 6) & (mggd['hour'] < 18)] #day merged gridded data\n",
    "nmggd = mggd[(mggd['hour'] < 6) | (mggd['hour'] >= 18)] #night merged gridded data\n",
    "fmggd = dmggd #choose focus merged gridded data\n",
    "\n",
    "tggd = fmggd.groupby(['mand_lvl', 'time_bin'])[target_columns].mean().reset_index() # 3.12\n",
    "tggd['time'] = pd.to_datetime(tggd['time_bin'], unit='D', origin=first_date)\n",
    "for col in target_columns:\n",
    "    tggd[col + '_uc_ucor_avg'] = fmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_ucor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.13\n",
    "    tggd[col + '_var'] = fmggd.groupby(['time_bin','mand_lvl'])[col].apply(\n",
    "                lambda x: ((((x-x.mean())**2).sum())/(len(x)*max((len(x)-1),1)))**0.5\n",
    "                ).reset_index(drop=True) #3.14\n",
    "    tggd[col + '_uc_sc']=fmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_scor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.15\n",
    "    tggd[col + '_uc_ucor']=(\n",
    "        tggd[col+'_uc_ucor_avg']**2 + tggd[col + '_var']**2 + tggd[col + '_uc_sc']**2)**0.5 #3.16\n",
    "    tggd[col + '_cor']=fmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_tcor'].mean().reset_index(drop=True) #3.17\n",
    "    tggd[col+'_uc']=(\n",
    "        tggd[col+'_uc_ucor']**2 + tggd[col+'_cor']**2)**0.5 #3.18\n",
    "    \n",
    "# Plotting the temporal gridded data\n",
    "for column in target_columns:\n",
    "    # Separate data into day and night based on time (e.g., 6:00-18:00 as day, else night)\n",
    "    mand_lvl_val = 1000\n",
    "    subset = tggd[tggd['mand_lvl'] == mand_lvl_val].copy()\n",
    "    subset['hour'] = subset['time'].dt.hour\n",
    "    # Also plot the original day_data for comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(day_data['time'], day_data[column], marker='x', linestyle='--', label=f'{column} (Original Day)')\n",
    "    ax.fill_between(day_data['time'], day_data[column] - day_data[column + '_uc'], day_data[column] + day_data[column + '_uc'], alpha=0.1, label='Original Day Uncertainty')\n",
    "    ax.plot(subset['time'], subset[column], marker='o', linestyle='-', label=f'{column} (Temporal Gridded)')\n",
    "    ax.fill_between(subset['time'], subset[column] - subset[column + '_uc'], subset[column] + subset[column + '_uc'], alpha=0.2, label='Uncertainty')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(column.capitalize())\n",
    "    ax.set_title(f'{column.capitalize()} at Mandatory Level {mand_lvl_val} Over Time (Temporal Gridded) at Day')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7166f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal gridding from scratch\n",
    "\n",
    "fmggd = nmggd #choose focus merged gridded data\n",
    "\n",
    "tggd = fmggd.groupby(['mand_lvl', 'time_bin'])[target_columns].mean().reset_index() # 3.12\n",
    "tggd['time'] = pd.to_datetime(tggd['time_bin'], unit='D', origin=first_date)\n",
    "for col in target_columns:\n",
    "    tggd[col + '_uc_ucor_avg'] = fmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_ucor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.13\n",
    "    tggd[col + '_var'] = fmggd.groupby(['time_bin','mand_lvl'])[col].apply(\n",
    "                lambda x: ((((x-x.mean())**2).sum())/(len(x)*max((len(x)-1),1)))**0.5\n",
    "                ).reset_index(drop=True) #3.14\n",
    "    tggd[col + '_uc_sc']=fmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_scor'].apply(\n",
    "                lambda x: (((x**2).sum())**0.5)/len(x)\n",
    "                ).reset_index(drop=True) #3.15\n",
    "    tggd[col + '_uc_ucor']=(\n",
    "        tggd[col+'_uc_ucor_avg']**2 + tggd[col + '_var']**2 + tggd[col + '_uc_sc']**2)**0.5 #3.16\n",
    "    tggd[col + '_cor']=fmggd.groupby(['time_bin','mand_lvl'])[col + '_uc_tcor'].mean().reset_index(drop=True) #3.17\n",
    "    tggd[col+'_uc']=(\n",
    "        tggd[col+'_uc_ucor']**2 + tggd[col+'_cor']**2)**0.5 #3.18\n",
    "    \n",
    "# Plotting the temporal gridded data\n",
    "for column in target_columns:\n",
    "    # Separate data into day and night based on time (e.g., 6:00-18:00 as day, else night)\n",
    "    mand_lvl_val = 1000\n",
    "    subset = tggd[tggd['mand_lvl'] == mand_lvl_val].copy()\n",
    "    subset['hour'] = subset['time'].dt.hour\n",
    "    # Also plot the original day_data for comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(day_data['time'], day_data[column], marker='x', linestyle='--', label=f'{column} (Original Day)')\n",
    "    ax.fill_between(day_data['time'], day_data[column] - day_data[column + '_uc'], day_data[column] + day_data[column + '_uc'], alpha=0.1, label='Original Day Uncertainty')\n",
    "    ax.plot(subset['time'], subset[column], marker='o', linestyle='-', label=f'{column} (Temporal Gridded)')\n",
    "    ax.fill_between(subset['time'], subset[column] - subset[column + '_uc'], subset[column] + subset[column + '_uc'], alpha=0.2, label='Uncertainty')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(column.capitalize())\n",
    "    ax.set_title(f'{column.capitalize()} at Mandatory Level {mand_lvl_val} Over Time (Temporal Gridded) at Night')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd50804",
   "metadata": {},
   "source": [
    "## PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "778b5be2",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a041fe6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
